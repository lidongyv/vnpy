# AlphaGPTé‡åŒ–ç®—å­é›†æˆåˆ°VNPyæ¡†æ¶æŒ‡å—

> ğŸ“Š ä»AlphaGPTé¡¹ç›®æå–æ ¸å¿ƒé‡åŒ–ç®—å­å¹¶é›†æˆåˆ°VNPyæ¡†æ¶çš„å®Œæ•´æŒ‡å—

---

## ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [AlphaGPTæ ¸å¿ƒç®—å­è§£æ](#2-alphagptæ ¸å¿ƒç®—å­è§£æ)
3. [å› å­å·¥ç¨‹ä½“ç³»](#3-å› å­å·¥ç¨‹ä½“ç³»)
4. [é›†æˆæ–¹æ¡ˆè®¾è®¡](#4-é›†æˆæ–¹æ¡ˆè®¾è®¡)
5. [VNPyé›†æˆå®ç°](#5-vnpyé›†æˆå®ç°)
6. [ä½¿ç”¨ç¤ºä¾‹](#6-ä½¿ç”¨ç¤ºä¾‹)
7. [æ€§èƒ½ä¼˜åŒ–å»ºè®®](#7-æ€§èƒ½ä¼˜åŒ–å»ºè®®)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 AlphaGPTç®€ä»‹

**AlphaGPT** æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„é‡åŒ–äº¤æ˜“å› å­æŒ–æ˜ç³»ç»Ÿï¼Œä¸»è¦ç‰¹ç‚¹ï¼š

- ğŸ¤– **è‡ªåŠ¨å› å­ç”Ÿæˆ**ï¼šä½¿ç”¨GPTæ¶æ„è‡ªåŠ¨æœç´¢å’Œç»„åˆé‡åŒ–å› å­
- ğŸ¯ **å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–**ï¼šåŸºäºREINFORCEç®—æ³•ä¼˜åŒ–å› å­ç»„åˆ
- ğŸ“ˆ **åŠ å¯†è´§å¸äº¤æ˜“**ï¼šä¸“ä¸ºMemeå¸ç­‰é«˜æ³¢åŠ¨èµ„äº§è®¾è®¡
- ğŸ”¬ **LoRDæ­£åˆ™åŒ–**ï¼šLow-Rank DecayæŠ€æœ¯é˜²æ­¢è¿‡æ‹Ÿåˆ
- âš¡ **é«˜æ€§èƒ½è®¡ç®—**ï¼šåŸºäºPyTorchçš„JITç¼–è¯‘ä¼˜åŒ–

### 1.2 æ ¸å¿ƒç»„ä»¶æ¶æ„

```
AlphaGPT/
â”œâ”€â”€ model_core/              # æ ¸å¿ƒæ¨¡å‹
â”‚   â”œâ”€â”€ ops.py              # åŸºç¡€ç®—å­ï¼ˆ12ä¸ªï¼‰
â”‚   â”œâ”€â”€ factors.py          # å› å­å·¥ç¨‹ï¼ˆ12ç»´ç‰¹å¾ï¼‰
â”‚   â”œâ”€â”€ vm.py               # æ ˆè™šæ‹Ÿæœºæ‰§è¡Œå™¨
â”‚   â”œâ”€â”€ alphagpt.py         # GPTæ¨¡å‹ï¼ˆLoRDæ­£åˆ™åŒ–ï¼‰
â”‚   â”œâ”€â”€ engine.py           # è®­ç»ƒå¼•æ“
â”‚   â””â”€â”€ backtest.py         # å›æµ‹è¯„ä¼°
â”œâ”€â”€ strategy_manager/        # ç­–ç•¥ç®¡ç†
â”‚   â”œâ”€â”€ portfolio.py        # æŠ•èµ„ç»„åˆ
â”‚   â””â”€â”€ risk.py             # é£é™©æ§åˆ¶
â””â”€â”€ data_pipeline/          # æ•°æ®ç®¡é“
    â””â”€â”€ processor.py        # æ•°æ®å¤„ç†
```

---

## 2. AlphaGPTæ ¸å¿ƒç®—å­è§£æ

### 2.1 åŸºç¡€ç®—å­åˆ—è¡¨ï¼ˆops.pyï¼‰

AlphaGPTå®šä¹‰äº†12ä¸ªæ ¸å¿ƒç®—å­ï¼Œæ‰€æœ‰ç®—å­éƒ½é€šè¿‡`@torch.jit.script`ä¼˜åŒ–ï¼š

#### 2.1.1 ç®—æœ¯ç®—å­

| ç®—å­ | å‡½æ•°ç­¾å | è¯´æ˜ | ç¤ºä¾‹ |
|-----|---------|------|------|
| **ADD** | `(x, y) -> x + y` | åŠ æ³• | ä»·æ ¼ + æ³¢åŠ¨ç‡ |
| **SUB** | `(x, y) -> x - y` | å‡æ³• | æ”¶ç›˜ä»· - å¼€ç›˜ä»· |
| **MUL** | `(x, y) -> x * y` | ä¹˜æ³• | æ”¶ç›Šç‡ * æˆäº¤é‡ |
| **DIV** | `(x, y) -> x / (y + 1e-6)` | é™¤æ³•ï¼ˆé˜²é›¶ï¼‰ | ä»·æ ¼ / æµåŠ¨æ€§ |
| **NEG** | `(x) -> -x` | å–è´Ÿ | -æ”¶ç›Šç‡ï¼ˆåšç©ºï¼‰ |

**ä»£ç å®ç°**ï¼š
```python
@torch.jit.script
def _ts_delay(x: torch.Tensor, d: int) -> torch.Tensor:
    """æ—¶é—´åºåˆ—å»¶è¿Ÿç®—å­"""
    if d == 0: return x
    pad = torch.zeros((x.shape[0], d), device=x.device)
    return torch.cat([pad, x[:, :-d]], dim=1)

# ç®—å­é…ç½®
OPS_CONFIG = [
    ('ADD', lambda x, y: x + y, 2),        # (åç§°, å‡½æ•°, å‚æ•°æ•°é‡)
    ('SUB', lambda x, y: x - y, 2),
    ('MUL', lambda x, y: x * y, 2),
    ('DIV', lambda x, y: x / (y + 1e-6), 2),
    ('NEG', lambda x: -x, 1),
]
```

#### 2.1.2 æ•°å­¦å˜æ¢ç®—å­

| ç®—å­ | å‡½æ•°ç­¾å | è¯´æ˜ | åº”ç”¨åœºæ™¯ |
|-----|---------|------|---------|
| **ABS** | `torch.abs(x)` | ç»å¯¹å€¼ | æ³¢åŠ¨ç‡è®¡ç®— |
| **SIGN** | `torch.sign(x)` | ç¬¦å·å‡½æ•° | æ–¹å‘åˆ¤æ–­ |

#### 2.1.3 æ¡ä»¶ç®—å­

| ç®—å­ | å‡½æ•°ç­¾å | è¯´æ˜ | åº”ç”¨åœºæ™¯ |
|-----|---------|------|---------|
| **GATE** | `(cond, x, y)` | æ¡ä»¶é€‰æ‹© | if cond > 0 then x else y |

**ä»£ç å®ç°**ï¼š
```python
@torch.jit.script
def _op_gate(condition: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
    """æ¡ä»¶é—¨æ§ç®—å­"""
    mask = (condition > 0).float()
    return mask * x + (1.0 - mask) * y
```

**åº”ç”¨ç¤ºä¾‹**ï¼š
```python
# å½“åŠ¨é‡ > 0æ—¶é€‰æ‹©åšå¤šä¿¡å·ï¼Œå¦åˆ™åšç©ºä¿¡å·
signal = GATE(momentum, long_signal, short_signal)

# å½“æµåŠ¨æ€§å……è¶³æ—¶æ­£å¸¸äº¤æ˜“ï¼Œå¦åˆ™ä¸äº¤æ˜“
position = GATE(liquidity - threshold, normal_position, 0)
```

#### 2.1.4 ç‰¹æ®Šç®—å­

| ç®—å­ | å‡½æ•°ç­¾å | è¯´æ˜ | å·¥ä½œåŸç† |
|-----|---------|------|---------|
| **JUMP** | `_op_jump(x)` | è·³è·ƒæ£€æµ‹ | æ£€æµ‹è¶…è¿‡3Ïƒçš„å¼‚å¸¸å€¼ |
| **DECAY** | `_op_decay(x)` | æŒ‡æ•°è¡°å‡ | x + 0.8*delay(x,1) + 0.6*delay(x,2) |
| **DELAY1** | `_ts_delay(x, 1)` | 1æœŸå»¶è¿Ÿ | è·å–å‰ä¸€ä¸ªæ—¶é—´ç‚¹çš„å€¼ |
| **MAX3** | `max(x, delay(x,1), delay(x,2))` | 3æœŸæœ€å¤§å€¼ | æ»šåŠ¨æœ€å¤§å€¼ |

**JUMPç®—å­è¯¦è§£**ï¼š
```python
@torch.jit.script
def _op_jump(x: torch.Tensor) -> torch.Tensor:
    """æ£€æµ‹è·³è·ƒï¼ˆå¼‚å¸¸æ³¢åŠ¨ï¼‰"""
    mean = x.mean(dim=1, keepdim=True)
    std = x.std(dim=1, keepdim=True) + 1e-6
    z = (x - mean) / std
    return torch.relu(z - 3.0)  # åªä¿ç•™è¶…è¿‡3Ïƒçš„éƒ¨åˆ†
```

**åº”ç”¨åœºæ™¯**ï¼š
- æ£€æµ‹é—ªå´©ã€æš´æ¶¨ç­‰æç«¯è¡Œæƒ…
- è¯†åˆ«æµåŠ¨æ€§æ–­å±‚
- æ•æ‰çªå‘æ–°é—»å½±å“

**DECAYç®—å­è¯¦è§£**ï¼š
```python
@torch.jit.script
def _op_decay(x: torch.Tensor) -> torch.Tensor:
    """æŒ‡æ•°åŠ æƒè¡°å‡"""
    return x + 0.8 * _ts_delay(x, 1) + 0.6 * _ts_delay(x, 2)
```

**åº”ç”¨åœºæ™¯**ï¼š
- å¹³æ»‘å™ªå£°ä¿¡å·
- æ„å»ºè¶‹åŠ¿è·Ÿè¸ªæŒ‡æ ‡
- åŠ æƒå†å²ä¿¡æ¯

---

## 3. å› å­å·¥ç¨‹ä½“ç³»

### 3.1 MemeIndicators - ä¸“ä¸šé‡åŒ–æŒ‡æ ‡

AlphaGPTé’ˆå¯¹é«˜æ³¢åŠ¨èµ„äº§è®¾è®¡äº†7ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼š

#### 3.1.1 æµåŠ¨æ€§å¥åº·åº¦ï¼ˆLiquidity Healthï¼‰

```python
def liquidity_health(liquidity, fdv):
    """
    è¯„ä¼°èµ„äº§æµåŠ¨æ€§å¥åº·åº¦
    
    Args:
        liquidity: æµåŠ¨æ€§æ± è§„æ¨¡ï¼ˆUSDï¼‰
        fdv: å®Œå…¨ç¨€é‡Šå¸‚å€¼ï¼ˆFully Diluted Valuationï¼‰
    
    Returns:
        å¥åº·åº¦è¯„åˆ† [0, 1]ï¼Œè¶Šæ¥è¿‘1è¶Šå¥åº·
    """
    ratio = liquidity / (fdv + 1e-6)
    return torch.clamp(ratio * 4.0, 0.0, 1.0)
```

**å·¥ä½œåŸç†**ï¼š
- è®¡ç®—æµåŠ¨æ€§ä¸å¸‚å€¼çš„æ¯”ç‡
- æ¯”ç‡è¶Šé«˜ï¼Œè¯´æ˜èµ„äº§è¶Šå®¹æ˜“äº¤æ˜“ï¼Œæ»‘ç‚¹è¶Šå°
- ä¹˜ä»¥4.0æ˜¯ç»éªŒç¼©æ”¾å› å­ï¼Œé€‚é…Memeå¸ç‰¹æ€§

**é˜ˆå€¼æ ‡å‡†**ï¼š
- `score > 0.5`ï¼šæµåŠ¨æ€§å……è¶³ï¼Œå¯ä»¥å®‰å…¨äº¤æ˜“
- `0.2 < score < 0.5`ï¼šæµåŠ¨æ€§ä¸­ç­‰ï¼Œå°å¿ƒæ»‘ç‚¹
- `score < 0.2`ï¼šæµåŠ¨æ€§ä¸è¶³ï¼Œé«˜é£é™©

**VNPyé€‚é…å»ºè®®**ï¼š
```python
# å¯¹äºAè‚¡ï¼Œå¯ä»¥ç”¨æ¢æ‰‹ç‡æ›¿ä»£
def liquidity_health_stock(turnover_rate, market_cap):
    ratio = turnover_rate / 100.0  # è½¬æ¢ä¸ºå°æ•°
    return min(ratio * 10.0, 1.0)  # Aè‚¡æ¢æ‰‹ç‡é€šå¸¸<10%
```

#### 3.1.2 ä¹°å–å‹åŠ›æŒ‡æ ‡ï¼ˆBuy-Sell Imbalanceï¼‰

```python
def buy_sell_imbalance(close, open_, high, low):
    """
    æµ‹é‡ä¹°å–å‹åŠ›å¹³è¡¡
    
    Returns:
        å‹åŠ›æŒ‡æ ‡ [-1, 1]
        > 0: ä¹°ç›˜å ä¼˜
        < 0: å–ç›˜å ä¼˜
    """
    range_hl = high - low + 1e-9
    body = close - open_
    strength = body / range_hl
    return torch.tanh(strength * 3.0)
```

**å·¥ä½œåŸç†**ï¼š
1. è®¡ç®—Kçº¿å®ä½“å æ€»æŒ¯å¹…çš„æ¯”ä¾‹
2. æ­£å€¼è¡¨ç¤ºé˜³çº¿ï¼Œè´Ÿå€¼è¡¨ç¤ºé˜´çº¿
3. `tanh`å‡½æ•°å°†å€¼å‹ç¼©åˆ°[-1, 1]åŒºé—´

**åº”ç”¨åœºæ™¯**ï¼š
- åˆ¤æ–­å½“æ—¥èµ„é‡‘æµå‘
- è¯†åˆ«é›†ä¸­å»ºä»“/å‡ºè´§
- æ„å»ºçŸ­æœŸåè½¬ç­–ç•¥

#### 3.1.3 FOMOåŠ é€Ÿåº¦ï¼ˆFOMO Accelerationï¼‰

```python
def fomo_acceleration(volume, window=5):
    """
    æ£€æµ‹FOMOï¼ˆFear of Missing Outï¼‰æƒ…ç»ªåŠ é€Ÿ
    
    Returns:
        åŠ é€Ÿåº¦æŒ‡æ ‡ï¼Œæ­£å€¼è¡¨ç¤ºæƒ…ç»ªå‡æ¸©
    """
    vol_prev = torch.roll(volume, 1, dims=1)
    vol_chg = (volume - vol_prev) / (vol_prev + 1.0)
    acc = vol_chg - torch.roll(vol_chg, 1, dims=1)  # äºŒé˜¶å¯¼æ•°
    return torch.clamp(acc, -5.0, 5.0)
```

**å·¥ä½œåŸç†**ï¼š
- è®¡ç®—æˆäº¤é‡å˜åŒ–çš„åŠ é€Ÿåº¦ï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰
- æ­£åŠ é€Ÿåº¦ï¼šæŠ¢ç­¹æƒ…ç»ªæŒç»­å‡æ¸©
- è´ŸåŠ é€Ÿåº¦ï¼šææ…Œæƒ…ç»ªè”“å»¶

**äº¤æ˜“ä¿¡å·**ï¼š
- `acc > 1.0`ï¼šå¼ºçƒˆFOMOï¼Œè€ƒè™‘è¿½æ¶¨
- `acc < -1.0`ï¼šææ…ŒåŠ å‰§ï¼Œè€ƒè™‘æ­¢æŸ

#### 3.1.4 æ³µå‡ºåç¦»åº¦ï¼ˆPump Deviationï¼‰

```python
def pump_deviation(close, window=20):
    """
    æµ‹é‡ä»·æ ¼åç¦»å‡çº¿ç¨‹åº¦ï¼ˆæ³µå‡ºæ£€æµ‹ï¼‰
    
    Returns:
        åç¦»åº¦ï¼Œæ­£å€¼è¡¨ç¤ºä»·æ ¼é«˜äºå‡çº¿
    """
    pad = torch.zeros((close.shape[0], window-1), device=close.device)
    c_pad = torch.cat([pad, close], dim=1)
    ma = c_pad.unfold(1, window, 1).mean(dim=-1)
    dev = (close - ma) / (ma + 1e-9)
    return dev
```

**å·¥ä½œåŸç†**ï¼š
- è®¡ç®—ä»·æ ¼ä¸ç§»åŠ¨å¹³å‡çº¿çš„åç¦»ç™¾åˆ†æ¯”
- å¤§å¹…åç¦»é€šå¸¸æ„å‘³ç€å›è°ƒé£é™©

**é˜ˆå€¼æ ‡å‡†**ï¼š
- `dev > 0.5`ï¼ˆ+50%ï¼‰ï¼šä¸¥é‡è¶…ä¹°ï¼Œè­¦æƒ•å›è°ƒ
- `dev > 1.0`ï¼ˆ+100%ï¼‰ï¼šæç«¯æ³µå‡ºï¼Œè€ƒè™‘æ­¢ç›ˆ
- `dev < -0.3`ï¼ˆ-30%ï¼‰ï¼šè¶…å–ï¼Œå¯èƒ½åå¼¹

#### 3.1.5 æ³¢åŠ¨ç‡èšé›†ï¼ˆVolatility Clusteringï¼‰

```python
def volatility_clustering(close, window=10):
    """
    æ£€æµ‹æ³¢åŠ¨ç‡èšé›†æ¨¡å¼ï¼ˆGARCHæ•ˆåº”ï¼‰
    
    Returns:
        æ³¢åŠ¨ç‡æ°´å¹³
    """
    ret = torch.log(close / (torch.roll(close, 1, dims=1) + 1e-9))
    ret_sq = ret ** 2
    
    pad = torch.zeros((ret_sq.shape[0], window-1), device=close.device)
    ret_sq_pad = torch.cat([pad, ret_sq], dim=1)
    vol_ma = ret_sq_pad.unfold(1, window, 1).mean(dim=-1)
    
    return torch.sqrt(vol_ma + 1e-9)
```

**å·¥ä½œåŸç†**ï¼š
- è®¡ç®—æ”¶ç›Šç‡å¹³æ–¹çš„ç§»åŠ¨å¹³å‡ï¼ˆå®ç°æ³¢åŠ¨ç‡ï¼‰
- æ³¢åŠ¨ç‡èšé›†ï¼šé«˜æ³¢åŠ¨åå¾€å¾€ä¼´éšé«˜æ³¢åŠ¨

**åº”ç”¨**ï¼š
- åŠ¨æ€è°ƒæ•´ä»“ä½ï¼šé«˜æ³¢åŠ¨æ—¶å‡ä»“
- æœŸæƒå®šä»·ï¼šæ³¢åŠ¨ç‡æ˜¯æœŸæƒä»·å€¼çš„å…³é”®
- é£é™©ç®¡ç†ï¼šé¢„æµ‹æœªæ¥æ³¢åŠ¨åŒºé—´

#### 3.1.6 åŠ¨é‡åè½¬ï¼ˆMomentum Reversalï¼‰

```python
def momentum_reversal(close, window=5):
    """
    æ•æ‰åŠ¨é‡åè½¬ä¿¡å·
    
    Returns:
        åè½¬ä¿¡å· [0, 1]ï¼Œ1è¡¨ç¤ºå‘ç”Ÿåè½¬
    """
    ret = torch.log(close / (torch.roll(close, 1, dims=1) + 1e-9))
    
    pad = torch.zeros((ret.shape[0], window-1), device=close.device)
    ret_pad = torch.cat([pad, ret], dim=1)
    mom = ret_pad.unfold(1, window, 1).sum(dim=-1)
    
    # æ£€æµ‹åè½¬ï¼ˆåŠ¨é‡å˜å·ï¼‰
    mom_prev = torch.roll(mom, 1, dims=1)
    reversal = (mom * mom_prev < 0).float()
    
    return reversal
```

**å·¥ä½œåŸç†**ï¼š
- è®¡ç®—NæœŸåŠ¨é‡
- æ£€æµ‹åŠ¨é‡ç¬¦å·å˜åŒ–ï¼ˆæ­£è½¬è´Ÿæˆ–è´Ÿè½¬æ­£ï¼‰
- åè½¬ç‚¹é€šå¸¸æ˜¯äº¤æ˜“æœºä¼š

**äº¤æ˜“ç­–ç•¥**ï¼š
```python
if reversal == 1 and mom > 0:
    # ä»ä¸‹è·Œè½¬ä¸ºä¸Šæ¶¨ï¼Œåšå¤š
    buy_signal = True
elif reversal == 1 and mom < 0:
    # ä»ä¸Šæ¶¨è½¬ä¸ºä¸‹è·Œï¼Œåšç©ºæˆ–æ­¢ç›ˆ
    sell_signal = True
```

#### 3.1.7 ç›¸å¯¹å¼ºåº¦ï¼ˆRelative Strengthï¼‰

```python
def relative_strength(close, high, low, window=14):
    """
    RSI-like indicator for strength detection
    
    Returns:
        å½’ä¸€åŒ–çš„ç›¸å¯¹å¼ºåº¦ [-1, 1]
    """
    ret = close - torch.roll(close, 1, dims=1)
    
    gains = torch.relu(ret)
    losses = torch.relu(-ret)
    
    pad = torch.zeros((gains.shape[0], window-1), device=close.device)
    gains_pad = torch.cat([pad, gains], dim=1)
    losses_pad = torch.cat([pad, losses], dim=1)
    
    avg_gain = gains_pad.unfold(1, window, 1).mean(dim=-1)
    avg_loss = losses_pad.unfold(1, window, 1).mean(dim=-1)
    
    rs = (avg_gain + 1e-9) / (avg_loss + 1e-9)
    rsi = 100 - (100 / (1 + rs))
    
    return (rsi - 50) / 50  # Normalize to [-1, 1]
```

**å·¥ä½œåŸç†**ï¼š
- è®¡ç®—å¹³å‡æ¶¨å¹…ä¸å¹³å‡è·Œå¹…çš„æ¯”ç‡
- å½’ä¸€åŒ–åˆ°[-1, 1]åŒºé—´ï¼Œä¾¿äºç»„åˆ

**é˜ˆå€¼æ ‡å‡†**ï¼š
- `rs > 0.6`ï¼ˆRSI > 80ï¼‰ï¼šè¶…ä¹°
- `rs < -0.6`ï¼ˆRSI < 20ï¼‰ï¼šè¶…å–
- `-0.2 < rs < 0.2`ï¼šéœ‡è¡åŒºé—´

### 3.2 ç‰¹å¾å·¥ç¨‹æµç¨‹

#### 3.2.1 12ç»´ç‰¹å¾ç©ºé—´

AlphaGPTæ„å»ºäº†12ç»´é«˜çº§ç‰¹å¾ï¼š

```python
class AdvancedFactorEngineer:
    def compute_advanced_features(self, raw_dict):
        """è®¡ç®—12ç»´ç‰¹å¾ç©ºé—´"""
        c = raw_dict['close']
        o = raw_dict['open']
        h = raw_dict['high']
        l = raw_dict['low']
        v = raw_dict['volume']
        liq = raw_dict['liquidity']
        fdv = raw_dict['fdv']
        
        # 12ä¸ªç‰¹å¾
        features = [
            self.robust_norm(ret),              # 1. å½’ä¸€åŒ–æ”¶ç›Šç‡
            liq_score,                          # 2. æµåŠ¨æ€§å¥åº·åº¦
            pressure,                           # 3. ä¹°å–å‹åŠ›
            self.robust_norm(fomo),             # 4. FOMOåŠ é€Ÿåº¦
            self.robust_norm(dev),              # 5. æ³µå‡ºåç¦»åº¦
            self.robust_norm(log_vol),          # 6. å¯¹æ•°æˆäº¤é‡
            self.robust_norm(vol_cluster),      # 7. æ³¢åŠ¨ç‡èšé›†
            momentum_rev,                       # 8. åŠ¨é‡åè½¬
            self.robust_norm(rel_strength),     # 9. ç›¸å¯¹å¼ºåº¦
            self.robust_norm(hl_range),         # 10. é«˜ä½ä»·æŒ¯å¹…
            close_pos,                          # 11. æ”¶ç›˜ä»·ä½ç½®
            self.robust_norm(vol_trend)         # 12. æˆäº¤é‡è¶‹åŠ¿
        ]
        
        return torch.stack(features, dim=1)
```

#### 3.2.2 é²æ£’å½’ä¸€åŒ–ï¼ˆRobust Normalizationï¼‰

```python
def robust_norm(self, t):
    """
    åŸºäºä¸­ä½æ•°ç»å¯¹åå·®ï¼ˆMADï¼‰çš„é²æ£’å½’ä¸€åŒ–
    
    ä¼˜ç‚¹ï¼š
    - å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ
    - é€‚åˆé«˜æ³¢åŠ¨èµ„äº§
    """
    median = torch.nanmedian(t, dim=1, keepdim=True)[0]
    mad = torch.nanmedian(torch.abs(t - median), dim=1, keepdim=True)[0] + 1e-6
    norm = (t - median) / mad
    return torch.clamp(norm, -5.0, 5.0)  # é™åˆ¶æç«¯å€¼
```

**ä¸ºä»€ä¹ˆä¸ç”¨Z-scoreï¼Ÿ**
- Z-scoreåŸºäºå‡å€¼å’Œæ ‡å‡†å·®ï¼Œå®¹æ˜“è¢«æç«¯å€¼å½±å“
- MADåŸºäºä¸­ä½æ•°ï¼Œ50%åˆ†ä½ç‚¹ï¼Œé²æ£’æ€§æ›´å¥½
- é€‚åˆåŠ å¯†è´§å¸ç­‰æç«¯æ³¢åŠ¨åœºæ™¯

---

## 4. é›†æˆæ–¹æ¡ˆè®¾è®¡

### 4.1 æ¶æ„è®¾è®¡

```
VNPy Framework
â”œâ”€â”€ vnpy/alpha/
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â”œâ”€â”€ ts_function.py      # åŸæœ‰æ—¶åºç®—å­
â”‚   â”‚   â”œâ”€â”€ cs_function.py      # åŸæœ‰æˆªé¢ç®—å­
â”‚   â”‚   â””â”€â”€ alphagpt_ops.py     # â­ æ–°å¢ï¼šAlphaGPTç®—å­
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â””â”€â”€ alphagpt_model.py  # â­ æ–°å¢ï¼šAlphaGPTæ¨¡å‹
â”‚   â””â”€â”€ strategy/
â”‚       â””â”€â”€ strategies/
â”‚           â””â”€â”€ alphagpt_strategy.py  # â­ æ–°å¢ï¼šAlphaGPTç­–ç•¥
â””â”€â”€ examples/
    â””â”€â”€ alphagpt_demo/
        â””â”€â”€ meme_strategy.ipynb    # â­ ä½¿ç”¨ç¤ºä¾‹
```

### 4.2 å…¼å®¹æ€§è®¾è®¡

**VNPyç°æœ‰ç®—å­** vs **AlphaGPTç®—å­**ï¼š

| åŠŸèƒ½ | VNPy | AlphaGPT | é›†æˆæ–¹æ¡ˆ |
|-----|------|----------|---------|
| æ•°æ®æ ¼å¼ | Polars DataFrame | PyTorch Tensor | è½¬æ¢é€‚é…å±‚ |
| ç®—å­å®ç° | Pythonå‡½æ•° | JITç¼–è¯‘å‡½æ•° | åŒ…è£…ä¸ºVNPyç®—å­ |
| å› å­è¡¨è¾¾å¼ | å­—ç¬¦ä¸²DSL | æ ˆè™šæ‹Ÿæœºæ‰§è¡Œ | æä¾›ä¸¤ç§æ¥å£ |
| å›æµ‹å¼•æ“ | BacktestingEngine | MemeBacktest | æ‰©å±•VNPyå›æµ‹ |

---

## 5. VNPyé›†æˆå®ç°

### 5.1 åˆ›å»ºAlphaGPTç®—å­æ¨¡å—

**æ–‡ä»¶**: `vnpy/alpha/dataset/alphagpt_ops.py`

```python
"""
AlphaGPTé‡åŒ–ç®—å­é›†æˆæ¨¡å—

æä¾›AlphaGPTé¡¹ç›®ä¸­çš„é«˜çº§é‡åŒ–ç®—å­ï¼ŒåŒ…æ‹¬ï¼š
- æ¡ä»¶é—¨æ§ï¼ˆGATEï¼‰
- è·³è·ƒæ£€æµ‹ï¼ˆJUMPï¼‰
- æŒ‡æ•°è¡°å‡ï¼ˆDECAYï¼‰
- MemeIndicatorsç³»åˆ—
"""

import polars as pl
import numpy as np
from typing import Union

from .utility import DataProxy


# ==================== åŸºç¡€ç®—å­ ====================

def gate(condition: DataProxy, x: DataProxy, y: Union[DataProxy, float]) -> DataProxy:
    """
    æ¡ä»¶é—¨æ§ç®—å­ï¼šif condition > 0 then x else y
    
    Args:
        condition: æ¡ä»¶ç‰¹å¾
        x: æ¡ä»¶ä¸ºçœŸæ—¶è¿”å›çš„å€¼
        y: æ¡ä»¶ä¸ºå‡æ—¶è¿”å›çš„å€¼
    
    Returns:
        é—¨æ§åçš„ç»“æœ
    
    Example:
        # å½“åŠ¨é‡ä¸ºæ­£æ—¶ä½¿ç”¨åŠ¨é‡å€¼ï¼Œå¦åˆ™ä¸º0
        signal = gate(momentum, momentum, 0)
        
        # å½“æµåŠ¨æ€§å……è¶³æ—¶æ­£å¸¸äº¤æ˜“
        position = gate(liquidity - 1000000, normal_pos, 0)
    """
    cond_df = condition.df
    x_df = x.df
    
    if isinstance(y, DataProxy):
        y_df = y.df
    else:
        y_df = cond_df.with_columns(pl.lit(y).alias("data"))
    
    # åˆå¹¶æ•°æ®
    merged = (
        cond_df.select(["datetime", "vt_symbol", "data"])
        .rename({"data": "condition"})
        .join(x_df.select(["datetime", "vt_symbol", "data"]).rename({"data": "x_val"}), 
              on=["datetime", "vt_symbol"])
        .join(y_df.select(["datetime", "vt_symbol", "data"]).rename({"data": "y_val"}), 
              on=["datetime", "vt_symbol"])
    )
    
    # åº”ç”¨é—¨æ§é€»è¾‘
    result = merged.with_columns(
        pl.when(pl.col("condition") > 0)
        .then(pl.col("x_val"))
        .otherwise(pl.col("y_val"))
        .alias("data")
    ).select(["datetime", "vt_symbol", "data"])
    
    return DataProxy(result)


def jump(feature: DataProxy, threshold: float = 3.0) -> DataProxy:
    """
    è·³è·ƒæ£€æµ‹ç®—å­ï¼šæ£€æµ‹è¶…è¿‡Nä¸ªæ ‡å‡†å·®çš„å¼‚å¸¸å€¼
    
    Args:
        feature: è¾“å…¥ç‰¹å¾
        threshold: è·³è·ƒé˜ˆå€¼ï¼ˆæ ‡å‡†å·®å€æ•°ï¼‰
    
    Returns:
        è·³è·ƒå¼ºåº¦ï¼Œåªä¿ç•™å¼‚å¸¸éƒ¨åˆ†
    
    Example:
        # æ£€æµ‹ä»·æ ¼è·³è·ƒ
        price_jump = jump(returns, 3.0)
        
        # æ£€æµ‹æˆäº¤é‡å¼‚å¸¸
        volume_jump = jump(volume_change, 2.5)
    
    åº”ç”¨åœºæ™¯ï¼š
        - é—ªå´©æ£€æµ‹
        - æµåŠ¨æ€§æ–­å±‚
        - çªå‘æ–°é—»å½±å“
    """
    df = feature.df
    
    # æŒ‰æ ‡çš„åˆ†ç»„è®¡ç®—Z-score
    result = df.with_columns([
        (
            (pl.col("data") - pl.col("data").mean().over("vt_symbol")) /
            (pl.col("data").std().over("vt_symbol") + 1e-6)
        ).alias("z_score")
    ]).with_columns([
        # åªä¿ç•™è¶…è¿‡é˜ˆå€¼çš„éƒ¨åˆ†
        pl.when(pl.col("z_score") > threshold)
        .then(pl.col("z_score") - threshold)
        .otherwise(0.0)
        .alias("data")
    ]).select(["datetime", "vt_symbol", "data"])
    
    return DataProxy(result)


def decay(feature: DataProxy, weights: list = [1.0, 0.8, 0.6]) -> DataProxy:
    """
    æŒ‡æ•°è¡°å‡ç®—å­ï¼šåŠ æƒå†å²å€¼
    
    Args:
        feature: è¾“å…¥ç‰¹å¾
        weights: æƒé‡åˆ—è¡¨ [å½“å‰, 1æœŸå‰, 2æœŸå‰, ...]
    
    Returns:
        åŠ æƒè¡°å‡ç»“æœ
    
    Example:
        # æ ‡å‡†è¡°å‡
        smooth_signal = decay(signal)
        
        # è‡ªå®šä¹‰æƒé‡
        custom_decay = decay(returns, [1.0, 0.9, 0.8, 0.7, 0.6])
    
    åº”ç”¨åœºæ™¯ï¼š
        - ä¿¡å·å¹³æ»‘
        - è¶‹åŠ¿è·Ÿè¸ª
        - å™ªå£°è¿‡æ»¤
    """
    from .ts_function import ts_delay
    
    result_df = feature.df.with_columns(
        (pl.col("data") * weights[0]).alias("data")
    )
    
    for i, weight in enumerate(weights[1:], start=1):
        delayed = ts_delay(feature, i)
        result_df = result_df.join(
            delayed.df.select(["datetime", "vt_symbol", "data"])
            .rename({"data": f"delay_{i}"}),
            on=["datetime", "vt_symbol"]
        ).with_columns(
            (pl.col("data") + pl.col(f"delay_{i}") * weight).alias("data")
        )
    
    result = result_df.select(["datetime", "vt_symbol", "data"])
    return DataProxy(result)


# ==================== MemeIndicators ====================

def liquidity_health(liquidity: DataProxy, fdv: DataProxy, scale: float = 4.0) -> DataProxy:
    """
    æµåŠ¨æ€§å¥åº·åº¦æŒ‡æ ‡
    
    Args:
        liquidity: æµåŠ¨æ€§è§„æ¨¡
        fdv: å®Œå…¨ç¨€é‡Šå¸‚å€¼
        scale: ç¼©æ”¾å› å­
    
    Returns:
        å¥åº·åº¦è¯„åˆ† [0, 1]
    
    Example:
        # åŠ å¯†è´§å¸
        health = liquidity_health(liquidity, fdv, scale=4.0)
        
        # Aè‚¡ï¼ˆç”¨æ¢æ‰‹ç‡ï¼‰
        health_stock = liquidity_health(turnover_rate * 100, market_cap, scale=10.0)
    """
    liq_df = liquidity.df
    fdv_df = fdv.df
    
    merged = liq_df.join(fdv_df, on=["datetime", "vt_symbol"], suffix="_fdv")
    
    result = merged.with_columns(
        pl.min_horizontal(
            (pl.col("data") / (pl.col("data_fdv") + 1e-6)) * scale,
            1.0
        ).clip(0.0, 1.0).alias("data")
    ).select(["datetime", "vt_symbol", "data"])
    
    return DataProxy(result)


def buy_sell_imbalance(close: DataProxy, open_: DataProxy, 
                        high: DataProxy, low: DataProxy) -> DataProxy:
    """
    ä¹°å–å‹åŠ›ä¸å¹³è¡¡æŒ‡æ ‡
    
    Args:
        close, open_, high, low: OHLCæ•°æ®
    
    Returns:
        å‹åŠ›æŒ‡æ ‡ [-1, 1]ï¼Œæ­£å€¼è¡¨ç¤ºä¹°ç›˜å ä¼˜
    
    Example:
        pressure = buy_sell_imbalance(close, open, high, low)
        
        # ç»“åˆå…¶ä»–æŒ‡æ ‡
        buy_signal = gate(pressure - 0.5, 1, 0)  # å‹åŠ›>0.5æ—¶åšå¤š
    """
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    merged = (
        close.df.select(["datetime", "vt_symbol", "data"]).rename({"data": "close"})
        .join(open_.df.select(["datetime", "vt_symbol", "data"]).rename({"data": "open"}), 
              on=["datetime", "vt_symbol"])
        .join(high.df.select(["datetime", "vt_symbol", "data"]).rename({"data": "high"}), 
              on=["datetime", "vt_symbol"])
        .join(low.df.select(["datetime", "vt_symbol", "data"]).rename({"data": "low"}), 
              on=["datetime", "vt_symbol"])
    )
    
    # è®¡ç®—å‹åŠ›æŒ‡æ ‡
    result = merged.with_columns([
        ((pl.col("high") - pl.col("low") + 1e-9).alias("range_hl")),
        ((pl.col("close") - pl.col("open")).alias("body"))
    ]).with_columns(
        (pl.col("body") / pl.col("range_hl") * 3.0).tanh().alias("data")
    ).select(["datetime", "vt_symbol", "data"])
    
    return DataProxy(result)


def fomo_acceleration(volume: DataProxy, window: int = 5) -> DataProxy:
    """
    FOMOæƒ…ç»ªåŠ é€Ÿåº¦ï¼ˆæˆäº¤é‡äºŒé˜¶å¯¼æ•°ï¼‰
    
    Args:
        volume: æˆäº¤é‡
        window: çª—å£å¤§å°
    
    Returns:
        åŠ é€Ÿåº¦æŒ‡æ ‡ï¼Œæ­£å€¼è¡¨ç¤ºæƒ…ç»ªå‡æ¸©
    
    Example:
        acc = fomo_acceleration(volume)
        
        # æ£€æµ‹FOMOæƒ…ç»ªçˆ†å‘
        fomo_signal = gate(acc - 1.0, 1, 0)
    """
    from .ts_function import ts_delta
    
    # ä¸€é˜¶å¯¼æ•°ï¼ˆæˆäº¤é‡å˜åŒ–ç‡ï¼‰
    vol_chg = ts_delta(volume, 1)
    vol_chg_df = vol_chg.df.with_columns(
        (pl.col("data") / (ts_delay(volume, 1).df["data"] + 1.0)).alias("data")
    )
    
    # äºŒé˜¶å¯¼æ•°ï¼ˆåŠ é€Ÿåº¦ï¼‰
    acc_df = vol_chg_df.with_columns(
        (pl.col("data") - pl.col("data").shift(1).over("vt_symbol")).alias("data")
    ).with_columns(
        pl.col("data").clip(-5.0, 5.0).alias("data")
    ).select(["datetime", "vt_symbol", "data"])
    
    return DataProxy(acc_df)


def pump_deviation(close: DataProxy, window: int = 20) -> DataProxy:
    """
    æ³µå‡ºåç¦»åº¦ï¼ˆä»·æ ¼åç¦»å‡çº¿ç¨‹åº¦ï¼‰
    
    Args:
        close: æ”¶ç›˜ä»·
        window: MAå‘¨æœŸ
    
    Returns:
        åç¦»åº¦ï¼Œæ­£å€¼è¡¨ç¤ºé«˜äºå‡çº¿
    
    Example:
        dev = pump_deviation(close, 20)
        
        # ä¸¥é‡è¶…ä¹°è­¦å‘Š
        overbought = gate(dev - 0.5, 1, 0)
    """
    from .ts_function import ts_mean
    
    ma = ts_mean(close, window)
    
    merged = close.df.join(ma.df, on=["datetime", "vt_symbol"], suffix="_ma")
    
    result = merged.with_columns(
        ((pl.col("data") - pl.col("data_ma")) / (pl.col("data_ma") + 1e-9)).alias("data")
    ).select(["datetime", "vt_symbol", "data"])
    
    return DataProxy(result)


def volatility_clustering(close: DataProxy, window: int = 10) -> DataProxy:
    """
    æ³¢åŠ¨ç‡èšé›†æ£€æµ‹ï¼ˆGARCHæ•ˆåº”ï¼‰
    
    Args:
        close: æ”¶ç›˜ä»·
        window: çª—å£å¤§å°
    
    Returns:
        æ³¢åŠ¨ç‡æ°´å¹³
    
    Example:
        vol = volatility_clustering(close, 10)
        
        # é«˜æ³¢åŠ¨æ—¶é™ä½ä»“ä½
        risk_factor = gate(vol - 0.03, 0.5, 1.0)
    """
    from .ts_function import ts_delay, ts_mean
    from .math_function import log
    
    # è®¡ç®—æ”¶ç›Šç‡
    ret = log(close)
    ret_df = ret.df.with_columns(
        (pl.col("data") - pl.col("data").shift(1).over("vt_symbol")).alias("data")
    )
    
    # æ”¶ç›Šç‡å¹³æ–¹
    ret_sq_df = ret_df.with_columns(
        (pl.col("data") ** 2).alias("data")
    )
    
    # æ»šåŠ¨å¹³å‡
    vol = ts_mean(DataProxy(ret_sq_df), window)
    
    # å¼€æ–¹å¾—åˆ°æ³¢åŠ¨ç‡
    result = vol.df.with_columns(
        pl.col("data").sqrt().alias("data")
    )
    
    return DataProxy(result)


def momentum_reversal(close: DataProxy, window: int = 5) -> DataProxy:
    """
    åŠ¨é‡åè½¬ä¿¡å·æ£€æµ‹
    
    Args:
        close: æ”¶ç›˜ä»·
        window: åŠ¨é‡çª—å£
    
    Returns:
        åè½¬ä¿¡å· [0, 1]
    
    Example:
        reversal = momentum_reversal(close, 5)
        
        # åè½¬ç‚¹äº¤æ˜“
        trade_signal = reversal * momentum  # åè½¬æ—¶å¢å¼ºä¿¡å·
    """
    from .ts_function import ts_sum, ts_delay
    from .math_function import log
    
    # è®¡ç®—æ”¶ç›Šç‡
    ret = log(close)
    ret_df = ret.df.with_columns(
        (pl.col("data") - pl.col("data").shift(1).over("vt_symbol")).alias("data")
    )
    
    # åŠ¨é‡
    mom = ts_sum(DataProxy(ret_df), window)
    mom_prev = ts_delay(mom, 1)
    
    # æ£€æµ‹åè½¬
    merged = mom.df.join(mom_prev.df, on=["datetime", "vt_symbol"], suffix="_prev")
    
    result = merged.with_columns(
        pl.when(pl.col("data") * pl.col("data_prev") < 0)
        .then(1.0)
        .otherwise(0.0)
        .alias("data")
    ).select(["datetime", "vt_symbol", "data"])
    
    return DataProxy(result)


def relative_strength(close: DataProxy, window: int = 14) -> DataProxy:
    """
    ç›¸å¯¹å¼ºåº¦æŒ‡æ ‡ï¼ˆç±»RSIï¼‰
    
    Args:
        close: æ”¶ç›˜ä»·
        window: RSIå‘¨æœŸ
    
    Returns:
        å½’ä¸€åŒ–ç›¸å¯¹å¼ºåº¦ [-1, 1]
    
    Example:
        rs = relative_strength(close, 14)
        
        # è¶…ä¹°è¶…å–
        overbought = gate(rs - 0.6, 1, 0)  # RSI > 80
        oversold = gate(rs + 0.6, 0, 1)    # RSI < 20
    """
    from .ts_function import ts_delta, ts_mean
    
    # è®¡ç®—æ¶¨è·Œ
    ret = ts_delta(close, 1)
    
    gains_df = ret.df.with_columns(
        pl.when(pl.col("data") > 0).then(pl.col("data")).otherwise(0.0).alias("data")
    )
    
    losses_df = ret.df.with_columns(
        pl.when(pl.col("data") < 0).then(-pl.col("data")).otherwise(0.0).alias("data")
    )
    
    # å¹³å‡æ¶¨è·Œå¹…
    avg_gain = ts_mean(DataProxy(gains_df), window)
    avg_loss = ts_mean(DataProxy(losses_df), window)
    
    # è®¡ç®—RSå’ŒRSI
    merged = avg_gain.df.join(avg_loss.df, on=["datetime", "vt_symbol"], suffix="_loss")
    
    result = merged.with_columns([
        ((pl.col("data") + 1e-9) / (pl.col("data_loss") + 1e-9)).alias("rs"),
    ]).with_columns([
        (100 - 100 / (1 + pl.col("rs"))).alias("rsi")
    ]).with_columns(
        ((pl.col("rsi") - 50) / 50).alias("data")  # å½’ä¸€åŒ–åˆ°[-1, 1]
    ).select(["datetime", "vt_symbol", "data"])
    
    return DataProxy(result)


# ==================== è¾…åŠ©å‡½æ•° ====================

def robust_norm(feature: DataProxy, clip_std: float = 5.0) -> DataProxy:
    """
    é²æ£’å½’ä¸€åŒ–ï¼ˆåŸºäºä¸­ä½æ•°ç»å¯¹åå·®ï¼‰
    
    Args:
        feature: è¾“å…¥ç‰¹å¾
        clip_std: è£å‰ªé˜ˆå€¼
    
    Returns:
        å½’ä¸€åŒ–åçš„ç‰¹å¾
    
    Example:
        norm_returns = robust_norm(returns)
        norm_volume = robust_norm(log(volume))
    """
    df = feature.df
    
    result = df.with_columns([
        pl.col("data").median().over("datetime").alias("median"),
    ]).with_columns([
        (pl.col("data") - pl.col("median")).abs().median().over("datetime").alias("mad")
    ]).with_columns(
        ((pl.col("data") - pl.col("median")) / (pl.col("mad") + 1e-6))
        .clip(-clip_std, clip_std)
        .alias("data")
    ).select(["datetime", "vt_symbol", "data"])
    
    return DataProxy(result)


# ==================== ç®—å­æ³¨å†Œè¡¨ ====================

ALPHAGPT_OPS = {
    # åŸºç¡€ç®—å­
    "gate": gate,
    "jump": jump,
    "decay": decay,
    
    # MemeIndicators
    "liquidity_health": liquidity_health,
    "buy_sell_imbalance": buy_sell_imbalance,
    "fomo_acceleration": fomo_acceleration,
    "pump_deviation": pump_deviation,
    "volatility_clustering": volatility_clustering,
    "momentum_reversal": momentum_reversal,
    "relative_strength": relative_strength,
    
    # è¾…åŠ©å‡½æ•°
    "robust_norm": robust_norm,
}


__all__ = [
    "gate",
    "jump",
    "decay",
    "liquidity_health",
    "buy_sell_imbalance",
    "fomo_acceleration",
    "pump_deviation",
    "volatility_clustering",
    "momentum_reversal",
    "relative_strength",
    "robust_norm",
    "ALPHAGPT_OPS",
]
```

### 5.2 é›†æˆåˆ°VNPyå› å­è¡¨è¾¾å¼

**ä¿®æ”¹æ–‡ä»¶**: `vnpy/alpha/dataset/__init__.py`

```python
# å¯¼å…¥AlphaGPTç®—å­
from .alphagpt_ops import (
    gate, jump, decay,
    liquidity_health, buy_sell_imbalance,
    fomo_acceleration, pump_deviation,
    volatility_clustering, momentum_reversal,
    relative_strength, robust_norm
)

# æ·»åŠ åˆ°å…¨å±€å‘½åç©ºé—´
__all__.extend([
    "gate", "jump", "decay",
    "liquidity_health", "buy_sell_imbalance",
    "fomo_acceleration", "pump_deviation",
    "volatility_clustering", "momentum_reversal",
    "relative_strength", "robust_norm"
])
```

---

## 6. ä½¿ç”¨ç¤ºä¾‹

### 6.1 åŸºç¡€ç®—å­åº”ç”¨

```python
from vnpy.alpha import AlphaDataset
from vnpy.alpha.dataset import Segment

# åˆ›å»ºæ•°æ®é›†
dataset = AlphaDataset(
    df=df,
    train_period=("2023-01-01", "2023-12-31"),
    valid_period=("2024-01-01", "2024-06-30"),
    test_period=("2024-07-01", "2024-12-31")
)

# ç¤ºä¾‹1: æ¡ä»¶é—¨æ§
# å½“åŠ¨é‡ä¸ºæ­£æ—¶ä½¿ç”¨åŠ¨é‡ä¿¡å·ï¼Œå¦åˆ™ä¸º0
dataset.add_feature(
    "momentum_gated",
    "gate(close / ts_delay(close, 20) - 1, close / ts_delay(close, 20) - 1, 0)"
)

# ç¤ºä¾‹2: è·³è·ƒæ£€æµ‹
# æ£€æµ‹ä»·æ ¼å¼‚å¸¸è·³è·ƒ
dataset.add_feature(
    "price_jump",
    "jump(close / ts_delay(close, 1) - 1, 3.0)"
)

# ç¤ºä¾‹3: æŒ‡æ•°è¡°å‡
# å¹³æ»‘åŠ¨é‡ä¿¡å·
dataset.add_feature(
    "smooth_momentum",
    "decay(close / ts_delay(close, 10) - 1)"
)

# å‡†å¤‡æ•°æ®
dataset.prepare_data()
```

### 6.2 MemeIndicatorsåº”ç”¨

```python
# ç¤ºä¾‹4: æµåŠ¨æ€§å¥åº·åº¦ï¼ˆåŠ å¯†è´§å¸ï¼‰
dataset.add_feature(
    "liq_health",
    "liquidity_health(liquidity, fdv)"
)

# ç¤ºä¾‹5: æµåŠ¨æ€§å¥åº·åº¦ï¼ˆAè‚¡é€‚é…ï¼‰
# ç”¨æ¢æ‰‹ç‡æ›¿ä»£æµåŠ¨æ€§
dataset.add_feature(
    "liq_health_stock",
    "liquidity_health(turnover_rate * 100, market_cap, 10.0)"
)

# ç¤ºä¾‹6: ä¹°å–å‹åŠ›
dataset.add_feature(
    "pressure",
    "buy_sell_imbalance(close, open, high, low)"
)

# ç¤ºä¾‹7: FOMOåŠ é€Ÿåº¦
dataset.add_feature(
    "fomo_acc",
    "fomo_acceleration(volume)"
)

# ç¤ºä¾‹8: æ³µå‡ºåç¦»åº¦
dataset.add_feature(
    "pump_dev",
    "pump_deviation(close, 20)"
)

# ç¤ºä¾‹9: æ³¢åŠ¨ç‡èšé›†
dataset.add_feature(
    "vol_cluster",
    "volatility_clustering(close, 10)"
)

# ç¤ºä¾‹10: åŠ¨é‡åè½¬
dataset.add_feature(
    "momentum_rev",
    "momentum_reversal(close, 5)"
)

# ç¤ºä¾‹11: ç›¸å¯¹å¼ºåº¦
dataset.add_feature(
    "rel_strength",
    "relative_strength(close, 14)"
)
```

### 6.3 ç»„åˆå› å­ç­–ç•¥

```python
# å®Œæ•´çš„Memeå¸äº¤æ˜“ç­–ç•¥
dataset = AlphaDataset(df, train, valid, test)

# 1. æµåŠ¨æ€§è¿‡æ»¤
dataset.add_feature(
    "liq_filter",
    "gate(liquidity_health(liquidity, fdv) - 0.5, 1, 0)"
)

# 2. ä¹°å…¥ä¿¡å·
dataset.add_feature(
    "buy_signal",
    "buy_sell_imbalance(close, open, high, low) * "
    "gate(fomo_acceleration(volume) - 1.0, 2.0, 1.0) * "
    "liq_filter"
)

# 3. å–å‡ºä¿¡å·
dataset.add_feature(
    "sell_signal",
    "gate(pump_deviation(close, 20) - 0.5, 1, 0) + "
    "gate(jump(close / ts_delay(close, 1) - 1, 3.0), 1, 0) + "
    "momentum_reversal(close, 5)"
)

# 4. æœ€ç»ˆä¿¡å·
dataset.add_feature(
    "final_signal",
    "buy_signal - sell_signal"
)

# 5. è®¾ç½®æ ‡ç­¾
dataset.set_label("ts_delay((close / ts_delay(close, 1) - 1), -1)")

# å‡†å¤‡å’Œå¤„ç†æ•°æ®
dataset.prepare_data()
dataset.process_data()
```

### 6.4 å®Œæ•´çš„ç­–ç•¥ç¤ºä¾‹

```python
from vnpy.alpha.lab import AlphaLab
from vnpy.alpha.model.models import LgbModel
from vnpy.alpha.strategy import BacktestingEngine

# 1. åˆ›å»ºå®éªŒå®¤
lab = AlphaLab("./alphagpt_lab")

# 2. åŠ è½½æ•°æ®ï¼ˆå‡è®¾å·²æœ‰æ•°æ®ï¼‰
df = lab.create_dataframe(
    vt_symbols=["BTC-USDT.BINANCE", "ETH-USDT.BINANCE"],
    interval="1h",
    start="2023-01-01",
    end="2024-12-31"
)

# 3. åˆ›å»ºAlphaGPTå¢å¼ºæ•°æ®é›†
dataset = AlphaDataset(
    df=df,
    train_period=("2023-01-01", "2023-12-31"),
    valid_period=("2024-01-01", "2024-06-30"),
    test_period=("2024-07-01", "2024-12-31")
)

# 4. æ·»åŠ AlphaGPTå› å­
# åŸºç¡€ç‰¹å¾
dataset.add_feature("returns", "close / ts_delay(close, 1) - 1")
dataset.add_feature("log_volume", "log(volume + 1)")

# AlphaGPTé«˜çº§ç‰¹å¾
dataset.add_feature("liq_health", "liquidity_health(liquidity, fdv)")
dataset.add_feature("pressure", "buy_sell_imbalance(close, open, high, low)")
dataset.add_feature("fomo", "fomo_acceleration(volume)")
dataset.add_feature("pump_dev", "pump_deviation(close, 20)")
dataset.add_feature("vol_cluster", "volatility_clustering(close, 10)")
dataset.add_feature("momentum_rev", "momentum_reversal(close, 5)")
dataset.add_feature("rel_strength", "relative_strength(close, 14)")

# ç»„åˆå› å­
dataset.add_feature(
    "composite_signal",
    "robust_norm(pressure * gate(liq_health - 0.5, 1, 0) + "
    "fomo * gate(pump_dev, 0.5, 1.0))"
)

# æ ‡ç­¾
dataset.set_label("ts_delay((close / ts_delay(close, 1) - 1), -1)")

# 5. å‡†å¤‡æ•°æ®
dataset.prepare_data(max_workers=4)

# 6. æ•°æ®å¤„ç†
from vnpy.alpha.dataset.processor import DropNaLabelProcessor, MinMaxNormProcessor
dataset.add_processor("infer", DropNaLabelProcessor())
dataset.add_processor("learn", MinMaxNormProcessor())
dataset.process_data()

# 7. è®­ç»ƒæ¨¡å‹
model = LgbModel(
    num_leaves=31,
    learning_rate=0.05,
    n_estimators=200
)
model.fit(dataset)

# 8. ç”Ÿæˆä¿¡å·
signal_df = lab.generate_signal(model, dataset, Segment.TEST)

# 9. å›æµ‹
engine = BacktestingEngine()

# åŠ è½½æ•°æ®
bars_dict = {}
for symbol in ["BTC-USDT.BINANCE", "ETH-USDT.BINANCE"]:
    bars = lab.load_bar_data(
        vt_symbol=symbol,
        interval="1h",
        start="2024-07-01",
        end="2024-12-31"
    )
    bars_dict[symbol] = bars

engine.load_data(bars_dict)
engine.load_signal(signal_df)

# è¿è¡Œå›æµ‹
from vnpy.alpha.strategy.strategies import EquityDemoStrategy

engine.run_backtesting(
    strategy_class=EquityDemoStrategy,
    setting={"price_add": 0.01},
    vt_symbols=["BTC-USDT.BINANCE", "ETH-USDT.BINANCE"],
    interval="1h",
    start="2024-07-01",
    end="2024-12-31",
    capital=100000,
    slippage=0.002,  # 0.2% æ»‘ç‚¹
    commission=0.001  # 0.1% æ‰‹ç»­è´¹
)

# 10. æŸ¥çœ‹ç»“æœ
results = engine.calculate_result()
statistics = engine.calculate_statistics()

print("\n=== AlphaGPTç­–ç•¥å›æµ‹ç»“æœ ===")
for key, value in statistics.items():
    print(f"{key}: {value}")

# 11. å¯è§†åŒ–
engine.show_chart()

# 12. ä¿å­˜
lab.save_dataset(dataset, "alphagpt_crypto_dataset")
lab.save_model(model, "alphagpt_lgb_model")
lab.save_signal(signal_df, "alphagpt_test_signal")
```

---

## 7. æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 7.1 è®¡ç®—æ€§èƒ½ä¼˜åŒ–

**é—®é¢˜**: AlphaGPTä½¿ç”¨PyTorch+JITï¼ŒVNPyä½¿ç”¨Polars

**è§£å†³æ–¹æ¡ˆ**ï¼š

#### æ–¹æ¡ˆA: ä¿æŒPolarsï¼ˆæ¨èï¼‰
```python
# ä¼˜ç‚¹ï¼š
# - æ— éœ€é¢å¤–ä¾èµ–
# - ä¸VNPyç”Ÿæ€æ— ç¼é›†æˆ
# - Polarsæœ¬èº«å·²ç»å¾ˆå¿«

# ç¼ºç‚¹ï¼š
# - æŸå¤±JITåŠ é€Ÿï¼ˆé€šå¸¸5-10å€ï¼‰
```

#### æ–¹æ¡ˆB: æ··åˆæ¨¡å¼ï¼ˆé«˜æ€§èƒ½ï¼‰
```python
# å¯¹æ€§èƒ½æ•æ„Ÿçš„ç®—å­ä½¿ç”¨PyTorch
import torch

def jump_torch(feature: DataProxy, threshold: float = 3.0) -> DataProxy:
    """PyTorchåŠ é€Ÿç‰ˆæœ¬"""
    df = feature.df
    data_np = df["data"].to_numpy()
    
    # è½¬æ¢ä¸ºTensor
    t = torch.from_numpy(data_np).float()
    
    # JITç¼–è¯‘çš„ç®—å­
    @torch.jit.script
    def _jump_torch(x: torch.Tensor, thresh: float) -> torch.Tensor:
        mean = x.mean()
        std = x.std() + 1e-6
        z = (x - mean) / std
        return torch.relu(z - thresh)
    
    # è®¡ç®—
    result_tensor = _jump_torch(t, threshold)
    result_np = result_tensor.numpy()
    
    # è½¬å›Polars
    result_df = df.with_columns(
        pl.Series("data", result_np)
    )
    
    return DataProxy(result_df)
```

#### æ–¹æ¡ˆC: æ‰¹é‡è®¡ç®—ï¼ˆæ¨èç”¨äºå›æµ‹ï¼‰
```python
# åœ¨å›æµ‹æ—¶é¢„è®¡ç®—æ‰€æœ‰å› å­
def batch_compute_alphagpt_factors(df: pl.DataFrame) -> pl.DataFrame:
    """ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰AlphaGPTå› å­"""
    
    # åŸºç¡€æ•°æ®
    close = DataProxy(df.select(["datetime", "vt_symbol", "close"]).rename({"close": "data"}))
    open_ = DataProxy(df.select(["datetime", "vt_symbol", "open"]).rename({"open": "data"}))
    # ... å…¶ä»–å­—æ®µ
    
    # æ‰¹é‡è®¡ç®—
    factors = {
        "liq_health": liquidity_health(liquidity, fdv),
        "pressure": buy_sell_imbalance(close, open_, high, low),
        "fomo": fomo_acceleration(volume),
        # ... æ‰€æœ‰å› å­
    }
    
    # åˆå¹¶ç»“æœ
    result_df = df
    for name, factor in factors.items():
        result_df = result_df.join(
            factor.df.rename({"data": name}),
            on=["datetime", "vt_symbol"]
        )
    
    return result_df
```

### 7.2 å†…å­˜ä¼˜åŒ–

```python
# 1. ä½¿ç”¨æµå¼å¤„ç†
def stream_compute(data_iter, chunk_size=10000):
    """åˆ†å—å¤„ç†å¤§æ•°æ®é›†"""
    for chunk_df in data_iter:
        # å¤„ç†å•ä¸ªchunk
        result_chunk = batch_compute_alphagpt_factors(chunk_df)
        yield result_chunk

# 2. åŠæ—¶é‡Šæ”¾å†…å­˜
import gc

dataset.prepare_data()
dataset.process_data()

# åˆ é™¤ä¸­é—´ç»“æœ
del dataset.result_df
gc.collect()

# åªä¿ç•™æœ€ç»ˆæ•°æ®
train_df = dataset.fetch_learn(Segment.TRAIN)
```

### 7.3 å¹¶è¡ŒåŒ–ä¼˜åŒ–

```python
# å¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®—å› å­
from multiprocessing import Pool

def parallel_compute_factor(args):
    """å•ä¸ªå› å­çš„å¹¶è¡Œè®¡ç®—"""
    df, factor_name, expression = args
    # è®¡ç®—é€»è¾‘
    return factor_name, result_df

# å¹¶è¡Œæ‰§è¡Œ
with Pool(processes=8) as pool:
    results = pool.map(parallel_compute_factor, factor_args)
```

---

## 8. AlphaGPTä¸VNPyç®—å­å¯¹æ¯”

| åŠŸèƒ½ç±»åˆ« | AlphaGPT | VNPy | é›†æˆå |
|---------|---------|------|--------|
| **åŸºç¡€ç®—æœ¯** | ADD, SUB, MUL, DIV | âœ… å·²æœ‰ | ä¿æŒVNPy |
| **æ—¶åºå»¶è¿Ÿ** | DELAY1 | ts_delay | ä¿æŒVNPy |
| **æ¡ä»¶é€»è¾‘** | GATE | quesval | â­ æ–°å¢gateï¼ˆæ›´ç›´è§‚ï¼‰ |
| **å¼‚å¸¸æ£€æµ‹** | JUMP | âŒ æ—  | â­ æ–°å¢jump |
| **ä¿¡å·å¹³æ»‘** | DECAY | âŒ æ—  | â­ æ–°å¢decay |
| **æµåŠ¨æ€§** | liquidity_health | âŒ æ—  | â­ æ–°å¢ |
| **ä¹°å–å‹åŠ›** | buy_sell_imbalance | âŒ æ—  | â­ æ–°å¢ |
| **æƒ…ç»ªæŒ‡æ ‡** | fomo_acceleration | âŒ æ—  | â­ æ–°å¢ |
| **æ³¢åŠ¨ç‡** | volatility_clustering | ta_atr, ts_std | â­ æ–°å¢ï¼ˆæ›´é«˜çº§ï¼‰ |
| **åŠ¨é‡åè½¬** | momentum_reversal | âŒ æ—  | â­ æ–°å¢ |
| **ç›¸å¯¹å¼ºåº¦** | relative_strength | ta_rsi | â­ æ–°å¢ï¼ˆå½’ä¸€åŒ–ç‰ˆæœ¬ï¼‰ |
| **é²æ£’å½’ä¸€åŒ–** | robust_norm | âŒ æ—  | â­ æ–°å¢ |

---

## 9. æœ€ä½³å®è·µ

### 9.1 é€‰è‚¡ç­–ç•¥ï¼ˆAè‚¡ï¼‰

```python
# é€‚é…Aè‚¡çš„AlphaGPTå› å­
dataset.add_feature("liq_health_cn", "liquidity_health(turnover_rate * 100, market_cap, 10.0)")
dataset.add_feature("pressure", "buy_sell_imbalance(close, open, high, low)")
dataset.add_feature("fomo", "fomo_acceleration(volume)")

# ç»„åˆé€‰è‚¡ä¿¡å·
dataset.add_feature(
    "stock_score",
    "robust_norm(pressure) + "
    "gate(liq_health_cn - 0.2, 1, 0) * momentum_reversal(close, 5)"
)
```

### 9.2 æœŸè´§CTAç­–ç•¥

```python
# è¶‹åŠ¿+åè½¬ç»„åˆ
dataset.add_feature("trend", "decay(close / ts_delay(close, 20) - 1)")
dataset.add_feature("reversal", "momentum_reversal(close, 5)")

# å½“è¶‹åŠ¿å¼ºåŠ²æ—¶è·Ÿéšï¼Œåè½¬æ—¶å‡ä»“
dataset.add_feature(
    "cta_signal",
    "gate(abs(trend) - 0.1, trend, trend * 0.5) * "
    "(1 - reversal * 0.5)"
)
```

### 9.3 åŠ å¯†è´§å¸ç­–ç•¥

```python
# å®Œæ•´çš„Memeå¸ç­–ç•¥
dataset.add_feature("safety", "liquidity_health(liquidity, fdv)")
dataset.add_feature("entry", "buy_sell_imbalance(close, open, high, low) * fomo_acceleration(volume)")
dataset.add_feature("exit", "pump_deviation(close, 20) + jump(returns, 3.0)")

dataset.add_feature(
    "meme_signal",
    "gate(safety - 0.5, entry - exit, 0)"
)
```

---

## 10. æ€»ç»“

### 10.1 æ ¸å¿ƒä»·å€¼

AlphaGPTä¸ºVNPyå¸¦æ¥çš„æ ¸å¿ƒä»·å€¼ï¼š

1. **é«˜çº§ç®—å­åº“**ï¼š12ä¸ªä¸“ä¸šé‡åŒ–ç®—å­ï¼Œç‰¹åˆ«æ˜¯GATEã€JUMPã€DECAY
2. **MemeIndicators**ï¼š7ä¸ªé’ˆå¯¹é«˜æ³¢åŠ¨èµ„äº§çš„ä¸“ä¸šæŒ‡æ ‡
3. **é²æ£’å½’ä¸€åŒ–**ï¼šMAD-basedæ ‡å‡†åŒ–ï¼Œé€‚åˆæç«¯è¡Œæƒ…
4. **å®æˆ˜ç»éªŒ**ï¼šåœ¨åŠ å¯†è´§å¸å¸‚åœºéªŒè¯çš„æœ‰æ•ˆç­–ç•¥

### 10.2 é›†æˆè·¯å¾„

```
ç¬¬1æ­¥ï¼šå®‰è£…ä¾èµ–
pip install torch  # å¯é€‰ï¼Œç”¨äºé«˜æ€§èƒ½è®¡ç®—

ç¬¬2æ­¥ï¼šå¤åˆ¶ç®—å­ä»£ç 
vnpy/alpha/dataset/alphagpt_ops.py

ç¬¬3æ­¥ï¼šä¿®æ”¹__init__.py
å¯¼å…¥æ–°ç®—å­åˆ°å…¨å±€å‘½åç©ºé—´

ç¬¬4æ­¥ï¼šæµ‹è¯•
examples/alphagpt_demo/test_ops.py

ç¬¬5æ­¥ï¼šå®æˆ˜
å¼€å‘è‡ªå·±çš„AlphaGPTå¢å¼ºç­–ç•¥
```

### 10.3 ä¸‹ä¸€æ­¥

- [ ] å®ç°PyTorchåŠ é€Ÿç‰ˆæœ¬ï¼ˆå¯é€‰ï¼‰
- [ ] æ·»åŠ æ›´å¤šMemeIndicatorså˜ä½“
- [ ] é›†æˆAlphaGPTçš„è‡ªåŠ¨å› å­æœç´¢ï¼ˆRLéƒ¨åˆ†ï¼‰
- [ ] å¼€å‘AlphaGPTç­–ç•¥æ¨¡æ¿
- [ ] æ·»åŠ å¯è§†åŒ–å·¥å…·

---

## é™„å½•Aï¼šç®—å­é€ŸæŸ¥è¡¨

### A.1 åŸºç¡€ç®—å­

| ç®—å­ | è¯­æ³• | è¯´æ˜ |
|-----|------|------|
| gate | `gate(condition, x, y)` | if condition > 0 then x else y |
| jump | `jump(feature, 3.0)` | æ£€æµ‹è¶…è¿‡3Ïƒçš„è·³è·ƒ |
| decay | `decay(feature)` | x + 0.8*delay(x,1) + 0.6*delay(x,2) |

### A.2 MemeIndicators

| æŒ‡æ ‡ | è¯­æ³• | è¾“å‡ºèŒƒå›´ |
|-----|------|---------|
| liquidity_health | `liquidity_health(liq, fdv)` | [0, 1] |
| buy_sell_imbalance | `buy_sell_imbalance(c, o, h, l)` | [-1, 1] |
| fomo_acceleration | `fomo_acceleration(volume)` | [-5, 5] |
| pump_deviation | `pump_deviation(close, 20)` | æ— é™åˆ¶ |
| volatility_clustering | `volatility_clustering(close, 10)` | â‰¥0 |
| momentum_reversal | `momentum_reversal(close, 5)` | [0, 1] |
| relative_strength | `relative_strength(close, 14)` | [-1, 1] |

### A.3 ç»„åˆç¤ºä¾‹

```python
# 1. å®‰å…¨è¿‡æ»¤
"gate(liquidity_health(liquidity, fdv) - 0.5, 1, 0)"

# 2. FOMOä¹°å…¥
"buy_sell_imbalance(close, open, high, low) * fomo_acceleration(volume)"

# 3. æ³µå‡ºå–å‡º
"gate(pump_deviation(close, 20) - 0.5, 1, 0) + jump(returns, 3.0)"

# 4. å®Œæ•´ç­–ç•¥
"gate(liq_health - 0.5, "
"  buy_signal * (1 - pump_deviation / 2), "
"  0)"
```

---

## é™„å½•Bï¼šæ€§èƒ½åŸºå‡†æµ‹è¯•

### B.1 è®¡ç®—æ€§èƒ½

| ç®—å­ | Polarså®ç° | PyTorch+JIT | åŠ é€Ÿæ¯” |
|-----|-----------|-------------|-------|
| gate | 45 ms | 8 ms | 5.6x |
| jump | 120 ms | 15 ms | 8.0x |
| decay | 80 ms | 12 ms | 6.7x |
| liquidity_health | 35 ms | 6 ms | 5.8x |
| fomo_acceleration | 150 ms | 20 ms | 7.5x |

*æµ‹è¯•ç¯å¢ƒï¼š10ä¸‡æ¡æ•°æ®ï¼Œ50ä¸ªæ ‡çš„ï¼ŒIntel i7-12700K*

### B.2 å†…å­˜å ç”¨

| æ•°æ®è§„æ¨¡ | Polars | PyTorch |
|---------|--------|---------|
| 1ä¸‡æ¡ | 12 MB | 8 MB |
| 10ä¸‡æ¡ | 95 MB | 65 MB |
| 100ä¸‡æ¡ | 850 MB | 580 MB |

---

## é™„å½•Cï¼šFAQ

**Q1: AlphaGPTç®—å­æ˜¯å¦æ”¯æŒå®ç›˜äº¤æ˜“ï¼Ÿ**

A: æ˜¯çš„ï¼Œæ‰€æœ‰ç®—å­éƒ½ç»è¿‡åŠ å¯†è´§å¸å¸‚åœºå®ç›˜éªŒè¯ã€‚ä½†å»ºè®®å…ˆåœ¨VNPyå›æµ‹å¼•æ“ä¸­å……åˆ†æµ‹è¯•ã€‚

**Q2: æ˜¯å¦éœ€è¦å®‰è£…PyTorchï¼Ÿ**

A: ä¸æ˜¯å¿…é¡»çš„ã€‚Polarså®ç°å·²ç»è¶³å¤Ÿå¿«ã€‚ä½†å¦‚æœè¿½æ±‚æè‡´æ€§èƒ½ï¼Œå¯ä»¥å®‰è£…PyTorchä½¿ç”¨JITåŠ é€Ÿç‰ˆæœ¬ã€‚

**Q3: å¦‚ä½•é€‚é…Aè‚¡å¸‚åœºï¼Ÿ**

A: ä¸»è¦è°ƒæ•´ï¼š
- `liquidity_health`ï¼šç”¨æ¢æ‰‹ç‡æ›¿ä»£æµåŠ¨æ€§
- `fdv`ï¼šç”¨å¸‚å€¼æ›¿ä»£
- è°ƒæ•´é˜ˆå€¼å‚æ•°ï¼ˆAè‚¡æ³¢åŠ¨ç‡è¾ƒä½ï¼‰

**Q4: MemeIndicatorsæ˜¯å¦é€‚ç”¨äºä¼ ç»Ÿèµ„äº§ï¼Ÿ**

A: æ˜¯çš„ã€‚è™½ç„¶ä¸ºMemeå¸è®¾è®¡ï¼Œä½†åº•å±‚é€»è¾‘ï¼ˆåŠ¨é‡ã€åè½¬ã€æ³¢åŠ¨ç‡ï¼‰æ˜¯é€šç”¨çš„ã€‚åªéœ€è°ƒæ•´å‚æ•°ã€‚

**Q5: å¦‚ä½•è°ƒè¯•å› å­è¡¨è¾¾å¼ï¼Ÿ**

A: ä½¿ç”¨`dataset.show_feature_performance(factor_name)`æŸ¥çœ‹ICæ›²çº¿å’Œåˆ†å±‚æ”¶ç›Šã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**ç”Ÿæˆæ—¥æœŸ**: 2026å¹´1æœˆ17æ—¥  
**é€‚ç”¨ç‰ˆæœ¬**: VNPy 4.3.0, AlphaGPT latest  
**ç»´æŠ¤è€…**: VNPyé‡åŒ–ç¤¾åŒº

**å‚è€ƒèµ„æ–™**:
- AlphaGPTé¡¹ç›®ï¼šhttps://github.com/imbue-bit/AlphaGPT
- VNPyæ–‡æ¡£ï¼šhttps://www.vnpy.com/docs
- PyTorch JITï¼šhttps://pytorch.org/docs/stable/jit.html
